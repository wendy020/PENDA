import torch
import torch.utils.data as data
import torch.optim as optim
import torch.nn as nn
import torch.autograd as autograd
from torch.autograd import Variable

import os
from collections import OrderedDict
import pickle
from glob import glob

import numpy as np
import pandas as pd
import random
import math

import matplotlib.pyplot as plt
from datetime import datetime
from scipy import stats
import torch_two_sample.statistics_diff as diff
from scipy.stats import norm
import time

from sklearn.model_selection import KFold, StratifiedKFold
from torch.utils.data import SubsetRandomSampler, DataLoader

from model import FAG

import sys
from os import path
sys.path.append( path.dirname( path.dirname( path.abspath(__file__) ) ) )
from HAGCN.model.model import Model
from data_loader.data import *

import argparse
from distutils.util import strtobool
from tqdm import tqdm

device = torch.device('cuda:0' if torch.cuda.is_available() else "cpu")

mean_angles = {"age": [0 for i in range(14)]}

def init_args():
    # add_argument use type=<type> to call <type>(arg)
    # so it don't work for bool, bool("str") always equal to True
    # use strtobool instead of bool

    parser = argparse.ArgumentParser(description='skelton generator args')
    # data loading arguments
    parser.add_argument('--train_folder', type=str, default='../data/3d/caiipe/skeleton_data/fail/', metavar='S',
                        help='train folder (default: ../data/3d/caiipe/skeleton_data/fail/)')
    parser.add_argument('--all_data', type=strtobool, default=False, metavar='B',
                        help='using pass and fail data (default: False)')
    parser.add_argument('--level', type=int, default=0, metavar='N',
                        help='level (default: 0)')
    parser.add_argument('--level_data', type=strtobool, default=False, metavar='B',
                        help='use level data (not AIMS) (default: False)')
    # dataset arguments
    parser.add_argument('--data_dim', type=int, default=3, metavar='N',
                        help='data dimension (default: 3)')
    parser.add_argument('--frames', type=int, default=96, metavar='N',
                        help='max frames (default: 96)')
    parser.add_argument('--stride', type=int, default=60, metavar='N',
                        help='strides of frames (default: 60)')
    parser.add_argument('--data_type', type=str, default="numpy", metavar='S',
                        help='data type (default: numpy)')
    parser.add_argument('--joint_num', type=int, default=13, metavar='N',
                        help='joint numbers (default: 13)')
    parser.add_argument('--age_hint', type=strtobool, default=True, metavar='B',
                        help='age hint (default: True)')
    parser.add_argument('--level_hint', type=strtobool, default=False, metavar='B',
                        help='level hint (default: False)')
    parser.add_argument('--angle_hint', type=strtobool, default=False, metavar='B',
                        help='mean angle hint (default: False)')
    parser.add_argument('--angle_diff_hint', type=strtobool, default=False, metavar='B',
                        help='angle difference hint (default: False)')
    parser.add_argument('--batch_size', type=int, default=128, metavar='N',
                        help='input batch size for training (default: 128)')
    parser.add_argument('--gsan_new_dir', type=str, default="../data/new_generated", metavar='S',
                        help='directory where place the new data generated by generator (default: ../data/new_generated)')

    # model arguments
    parser.add_argument('--in_channels', type=int, default=4, metavar='N',
                        help='in channels (default: 4)')
    parser.add_argument('--out_channels', type=int, default=300, metavar='N',
                        help='out channels (default: 300)')
    parser.add_argument('--path', type=str, default='weights/flgsan_fail_fold0.pth', metavar='S',
                        help='path of gsan pass model weight (default: weights/flgsan_fail_fold0.pth)')
    parser.add_argument('--fake_real', type=float, default=1, metavar='N',
                        help='fake real ratio (default: 1)')
    parser.add_argument('--alpha', type=float, default=0.5, metavar='N',
                        help='alpha of GSAN (default: 0.5)')
    args = parser.parse_args()

    return args

def parsing_path(path):
    name = os.path.splitext(os.path.basename(path))[0]
    ID = name.split('_')[0]
    if len(ID) > 3:
        ID = 0
    else:
        ID = int(ID)
    if name.find("_") == -1:
        age = 1
    else:
        age = name.split('_')[1]
        age = float(age.split('m')[0])
        if age > 14:
            age = 1
        else:
            age = int(age)
    
    level = int(path.split("/level")[1][:1])
    level = 2 if level == 3 else level

    return {"id": ID, "age": age, "level": level}

def compute_angle(clips):
    # (frames, num_point, data_dim)
    nose = clips[:,6,:]
    shoulder_mid = (clips[:,7,:] + clips[:,10,:]) / 2
    hip_mid = (clips[:,0,:] + clips[:,3,:]) / 2
        
    mean_angle = 0
    for i in range(clips.shape[1]):
        mean_angle += angle(nose[i,:], shoulder_mid[i,:], hip_mid[i,:])
    mean_angle /= clips.shape[1]

    return mean_angle

def compute_info(data, mode):
    mean_ang = [compute_angle(data[i]) for i in range(len(data))]
    infos = []
        
    for ang in mean_ang:
        # compute distance
        distance = []
        for i in range(len(mean_angles[mode])):
            distance.append((ang - mean_angles[mode][i]) * (ang - mean_angles[mode][i]))
        infos.append(distance.index(min(distance)))
        if mode == "age":
            infos[-1] += 1
        if mode == "level" and infos[-1] == 2:
            infos[-1] = 3
        
    return infos

def generate_with_gsan(dataloader):
    aug_data = []
    aug_label = []

    generator = FAG(num_class=1, num_point=args.joint_num, num_person=1, in_channels=args.data_dim+1,
                 out_channels=args.out_channels, frames=args.frames, alpha=args.alpha)
    PATH = args.path
    generator.load_state_dict(torch.load(PATH))
    generator.to(device)

    for i, data in enumerate(dataloader, 0):
        inputs, labels, names, paths, infos = data
        condition = infos["age"].to(device)
        inputs = inputs.to(device)
        labels = labels.to(device)
        # (1, data_dim, frames, num_point, num_person)
        inputs = inputs.permute(0, 4, 2, 3, 1)

        # generate new pass and fail skeleton
        new_data_size = int(len(inputs)*args.fake_real)

        new_data = []
        while True:
            if len(new_data) < new_data_size:
                new_data_t, mean, logvar = generator(inputs, condition)
                new_data += list(new_data_t)

            if len(new_data) >= new_data_size:
                break
    
        new_data = torch.stack(new_data)[:new_data_size].permute(4, 0, 2, 3, 1)

        new_data = new_data.cpu().detach().numpy()[0]
        # (1, data_dim, frames, num_point, num_person)

        # generate new file name and path
        # shuffle origin data paths
        index = [i for i in range(len(paths))]
        np.random.shuffle(index)
        shuffle_data_paths = [paths[i] for i in index]

        # parsing id, age, level
        data_infos = []
        for j in range(len(new_data)):
            data_infos.append(parsing_path(shuffle_data_paths[j]))

        # choose age
        age_data = compute_info(new_data, "age")
            
        # build new paths
        # Kuei_level_data/generated/fold{}/{name}.npy
        new_data_paths = ["{}/{}_{}m_Pull_to_sit_generated{}.npy".
                            format(args.gsan_new_dir, info["id"], age, idx) for info, age, idx in
                                                                            zip(data_infos, age_data, range(len(new_data)))]

        # store new data
        for j in range(len(new_data)):
            with open(new_data_paths[j], "wb") as f: # write binary file
                np.save(f, new_data[j], allow_pickle=True)

        print("generate {} skeletons".format(len(new_data)))
        for i in range(3):
            print("[{}] {}".format(i, new_data_paths[i]))

def get_modalities():
    modalities = ["skeleton"]
    if args.age_hint:
        modalities.append("age")
    if args.level_hint:
        modalities.append("level")
    if args.angle_hint:
        modalities.append("angle")
    if args.angle_diff_hint:
        modalities.append("angle-diff")
        
    return modalities

if __name__ == "__main__":
    args = init_args()

    train_folder = args.train_folder + "level{}".format(args.level)
    if args.all_data:
        train_folder = train_folder + "/*"
    # data
    print("load data from {}".format(train_folder))
    sample = load_data(train_folder)

    labels = 0
    if args.level_data:
        labels = [args.level for i in range(len(sample))]
    else:
        label_str = sample[0].split("/")[5]
        labels = 0
        if label_str == "pass":
            labels = [1 for i in range(len(sample))]
        else:
            labels = [0 for i in range(len(sample))]
    
    modalities = get_modalities()

    if args.level_data:
        dataset = TestLevelDataset(file_list=sample, label_list=labels, dim=args.data_dim, max_frames=args.frames, stride=args.stride,
                        data_type=args.data_type, joint_num=args.joint_num, age_hint=args.age_hint, level_hint=args.level_hint,
                        angle_hint=args.angle_hint, angle_diff_hint=args.angle_diff_hint, modalities=modalities)
    else:
        dataset = TestAIMSCDataset(file_list=sample, label_list=labels, dim=args.data_dim, max_frames=args.frames, stride=args.stride,
                        data_type=args.data_type, joint_num=args.joint_num, age_hint=args.age_hint, level_hint=args.level_hint,
                        angle_hint=args.angle_hint, angle_diff_hint=args.angle_diff_hint, modalities=modalities)
        
    for i in range(14):
        mean_angles["age"][i] = dataset.means["age"][i]
    
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)

    generate_with_gsan(dataloader)
